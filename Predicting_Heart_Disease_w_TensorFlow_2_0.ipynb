{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting Heart Disease w/ TensorFlow 2.0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnAntonusMaximus/predicting-heart-disease/blob/master/Predicting_Heart_Disease_w_TensorFlow_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-U36S0owkNQ",
        "colab_type": "text"
      },
      "source": [
        "#Predicting Heart Disease w/ Machine Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIosVNz0J1a",
        "colab_type": "text"
      },
      "source": [
        "### Advances in artificial intelligence is allowing us to give better tooling to healthcare providers around the world, from predicting cancer, to analyzing human genome sequences. Using data from Cleveland Clinic, I wanted to try analyzing patient datasets on heart disease to see how machine learning could potentially help cardiologists. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQTtYmhX0jeG",
        "colab_type": "text"
      },
      "source": [
        "### Relevant Information:\n",
        "     This database contains 76 attributes, but all published experiments\n",
        "     refer to using a subset of 14 of them.  In particular, the Cleveland\n",
        "     database is the only one that has been used by ML researchers to \n",
        "     this date.  The \"goal\" field refers to the presence of heart disease\n",
        "     in the patient.  It is integer valued from 0 (no presence) to 4.\n",
        "     Experiments with the Cleveland database have concentrated on simply\n",
        "     attempting to distinguish presence (values 1,2,3,4) from absence (value\n",
        "     0).  \n",
        "   \n",
        "     The names and social security numbers of the patients were recently \n",
        "     removed from the database, replaced with dummy values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goc1r4VMJvsH",
        "colab_type": "text"
      },
      "source": [
        "### Import All Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbsZHGYdI3mB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0.alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbul0xKCJHs7",
        "colab_type": "code",
        "outputId": "2f09628e-dc0e-4705-c4b1-453f29cbb7c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "tf.__version__\n",
        "print(\"Python Version: {}\".format(sys.version))\n",
        "print(\"SKLearn Version: {}\".format(sklearn.__version__))\n",
        "print(\"Pandas Version: {}\".format(pd.__version__))\n",
        "print(\"Numpy Version: {}\".format(np.__version__))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python Version: 3.6.8 (default, Jan 14 2019, 11:02:34) \n",
            "[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\n",
            "SKLearn Version: 0.21.3\n",
            "Pandas Version: 0.24.2\n",
            "Numpy Version: 1.16.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRikn68iLV-t",
        "colab_type": "text"
      },
      "source": [
        "## Import The Dataset From Cleveland Clinic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aF4CR5rKIX8",
        "colab_type": "code",
        "outputId": "db02e294-3c09-47d0-d845-e17008db6f90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "\n",
        "## Define Columns in Dataset\n",
        "\n",
        "names = ['age',\n",
        "        'sex',\n",
        "        'cp',\n",
        "        'trestbps',\n",
        "        'chol',\n",
        "        'fbs',\n",
        "        'restecg',\n",
        "        'thalach',\n",
        "        'exang',\n",
        "        'oldpeak',\n",
        "        'slope',\n",
        "        'ca',\n",
        "        'thal',\n",
        "        'class']\n",
        "\n",
        "cleveland_data = pd.read_csv(url, names=names)\n",
        "\n",
        "## Check the data\n",
        "cleveland_data.head()"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>233.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>67.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>286.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>67.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>250.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>41.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>204.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    age  sex   cp  trestbps   chol  fbs  ...  exang  oldpeak  slope   ca  thal class\n",
              "0  63.0  1.0  1.0     145.0  233.0  1.0  ...    0.0      2.3    3.0  0.0   6.0     0\n",
              "1  67.0  1.0  4.0     160.0  286.0  0.0  ...    1.0      1.5    2.0  3.0   3.0     2\n",
              "2  67.0  1.0  4.0     120.0  229.0  0.0  ...    1.0      2.6    2.0  2.0   7.0     1\n",
              "3  37.0  1.0  3.0     130.0  250.0  0.0  ...    0.0      3.5    3.0  0.0   3.0     0\n",
              "4  41.0  0.0  2.0     130.0  204.0  0.0  ...    0.0      1.4    1.0  0.0   3.0     0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMilDKCJ1CZq",
        "colab_type": "text"
      },
      "source": [
        "### Here we have columns of various patient features, things like resting ECG, cholesterol, stress test results, and other relevant features for making a heart disease diagnosis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niieScUzMUGL",
        "colab_type": "code",
        "outputId": "7cc2dc5c-220e-4db2-af46-c2d3f33dbc9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(cleveland_data.shape)\n",
        "print(cleveland_data.loc[1])"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n",
            "age          67\n",
            "sex           1\n",
            "cp            4\n",
            "trestbps    160\n",
            "chol        286\n",
            "fbs           0\n",
            "restecg       2\n",
            "thalach     108\n",
            "exang         1\n",
            "oldpeak     1.5\n",
            "slope         2\n",
            "ca          3.0\n",
            "thal        3.0\n",
            "class         2\n",
            "Name: 1, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c42ywbjW1TzY",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOnHZhIe1Xcu",
        "colab_type": "text"
      },
      "source": [
        "### Some of the data has missing or null values which will be useless in training a neural network, we need to do some data cleaning before we pass them into a neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDsFpcyAMtK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Remove missing data\n",
        "data = cleveland_data[-cleveland_data.isin(['?'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRRJ0ZKwOOcL",
        "colab_type": "code",
        "outputId": "5f327579-0e89-4954-fc95-444fa3392cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>45.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>264.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>57.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>57.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>38.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  sex   cp  trestbps   chol  ...  oldpeak  slope   ca  thal  class\n",
              "298  45.0  1.0  1.0     110.0  264.0  ...      1.2    2.0  0.0   7.0      1\n",
              "299  68.0  1.0  4.0     144.0  193.0  ...      3.4    2.0  2.0   7.0      2\n",
              "300  57.0  1.0  4.0     130.0  131.0  ...      1.2    2.0  1.0   7.0      3\n",
              "301  57.0  0.0  2.0     130.0  236.0  ...      0.0    2.0  1.0   3.0      1\n",
              "302  38.0  1.0  3.0     138.0  175.0  ...      0.0    1.0  NaN   3.0      0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlAubgTLOVo4",
        "colab_type": "code",
        "outputId": "928c1084-0179-407f-bdc9-bd280f40f0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "## Drop rows with NaN \n",
        "data = data.dropna(axis=0)\n",
        "data.tail()"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>57.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>45.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>264.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>57.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>57.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  sex   cp  trestbps   chol  ...  oldpeak  slope   ca  thal  class\n",
              "297  57.0  0.0  4.0     140.0  241.0  ...      0.2    2.0  0.0   7.0      1\n",
              "298  45.0  1.0  1.0     110.0  264.0  ...      1.2    2.0  0.0   7.0      1\n",
              "299  68.0  1.0  4.0     144.0  193.0  ...      3.4    2.0  2.0   7.0      2\n",
              "300  57.0  1.0  4.0     130.0  131.0  ...      1.2    2.0  1.0   7.0      3\n",
              "301  57.0  0.0  2.0     130.0  236.0  ...      0.0    2.0  1.0   3.0      1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBw35BhQ1iCM",
        "colab_type": "text"
      },
      "source": [
        "### All null values are dropped, leaving us with about 300 usefull patient records with complete information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAgEB2cnOmMu",
        "colab_type": "code",
        "outputId": "53569cd0-703a-4414-c512-d44d3a46ee79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "data.isnull().sum(axis = 0)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "cp          0\n",
              "trestbps    0\n",
              "chol        0\n",
              "fbs         0\n",
              "restecg     0\n",
              "thalach     0\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       0\n",
              "ca          0\n",
              "thal        0\n",
              "class       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxjiMkfJOpU_",
        "colab_type": "code",
        "outputId": "5a77ee15-7e6e-4c29-8440-729aa3f6e966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(data.shape)\n",
        "print(data.dtypes)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(297, 14)\n",
            "age         float64\n",
            "sex         float64\n",
            "cp          float64\n",
            "trestbps    float64\n",
            "chol        float64\n",
            "fbs         float64\n",
            "restecg     float64\n",
            "thalach     float64\n",
            "exang       float64\n",
            "oldpeak     float64\n",
            "slope       float64\n",
            "ca           object\n",
            "thal         object\n",
            "class         int64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw0OPETL1p5M",
        "colab_type": "text"
      },
      "source": [
        "### Our data types need to be changed to numerics before we pass them in, some more pre-processing..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr2HlGJuefpT",
        "colab_type": "code",
        "outputId": "4a7e84ff-f68e-4e30-a604-efa005af110a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "data = data.apply(pd.to_numeric)\n",
        "print(data.dtypes)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age         float64\n",
            "sex         float64\n",
            "cp          float64\n",
            "trestbps    float64\n",
            "chol        float64\n",
            "fbs         float64\n",
            "restecg     float64\n",
            "thalach     float64\n",
            "exang       float64\n",
            "oldpeak     float64\n",
            "slope       float64\n",
            "ca          float64\n",
            "thal        float64\n",
            "class         int64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPQX2zvQfRIM",
        "colab_type": "text"
      },
      "source": [
        "### Show the Data Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSSm9rSl11up",
        "colab_type": "text"
      },
      "source": [
        "### Now that our dataset is a little cleaner, we can view the distributions and see our dataset is skewed, has outliers, and how it may affect a predictive model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIDibX9weqpi",
        "colab_type": "code",
        "outputId": "6533e2e1-b7f3-4dc5-f279-579c2feaceab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "      <td>297.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>54.542088</td>\n",
              "      <td>0.676768</td>\n",
              "      <td>3.158249</td>\n",
              "      <td>131.693603</td>\n",
              "      <td>247.350168</td>\n",
              "      <td>0.144781</td>\n",
              "      <td>0.996633</td>\n",
              "      <td>149.599327</td>\n",
              "      <td>0.326599</td>\n",
              "      <td>1.055556</td>\n",
              "      <td>1.602694</td>\n",
              "      <td>0.676768</td>\n",
              "      <td>4.730640</td>\n",
              "      <td>0.946128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.049736</td>\n",
              "      <td>0.468500</td>\n",
              "      <td>0.964859</td>\n",
              "      <td>17.762806</td>\n",
              "      <td>51.997583</td>\n",
              "      <td>0.352474</td>\n",
              "      <td>0.994914</td>\n",
              "      <td>22.941562</td>\n",
              "      <td>0.469761</td>\n",
              "      <td>1.166123</td>\n",
              "      <td>0.618187</td>\n",
              "      <td>0.938965</td>\n",
              "      <td>1.938629</td>\n",
              "      <td>1.234551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>48.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>211.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>133.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>56.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>243.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>153.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>276.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>166.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              age         sex          cp  ...          ca        thal       class\n",
              "count  297.000000  297.000000  297.000000  ...  297.000000  297.000000  297.000000\n",
              "mean    54.542088    0.676768    3.158249  ...    0.676768    4.730640    0.946128\n",
              "std      9.049736    0.468500    0.964859  ...    0.938965    1.938629    1.234551\n",
              "min     29.000000    0.000000    1.000000  ...    0.000000    3.000000    0.000000\n",
              "25%     48.000000    0.000000    3.000000  ...    0.000000    3.000000    0.000000\n",
              "50%     56.000000    1.000000    3.000000  ...    0.000000    3.000000    0.000000\n",
              "75%     61.000000    1.000000    4.000000  ...    1.000000    7.000000    2.000000\n",
              "max     77.000000    1.000000    4.000000  ...    3.000000    7.000000    4.000000\n",
              "\n",
              "[8 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBSX-pI64SRz",
        "colab_type": "text"
      },
      "source": [
        "### Because the scale of our data has wide minimums and maximums, it may make sense for us to normalize our data (Scale all of our non-binary values to between 0 and 1 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BIjdoKz3g2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "923c2fe8-93c4-47b1-e15b-9395b1f6e53e"
      },
      "source": [
        "cols_to_norm = ['age', 'trestbps','chol', 'thalach', 'thal']\n",
        "\n",
        "data_normalized = data[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
        "\n",
        "## Add back in our other columns\n",
        "\n",
        "data_normalized['sex'] = data['sex']\n",
        "data_normalized['cp'] = data['cp']\n",
        "data_normalized['fbs'] = data['fbs']\n",
        "data_normalized['restecg'] = data['restecg']\n",
        "data_normalized['exang'] = data['exang']\n",
        "data_normalized['oldpeak'] = data['oldpeak']\n",
        "data_normalized['slope'] = data['slope']\n",
        "data_normalized['ca'] = data['ca']\n",
        "data_normalized['class'] = data['class']\n",
        "data_normalized"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>thalach</th>\n",
              "      <th>thal</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.481132</td>\n",
              "      <td>0.244292</td>\n",
              "      <td>0.603053</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.622642</td>\n",
              "      <td>0.365297</td>\n",
              "      <td>0.282443</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.235160</td>\n",
              "      <td>0.442748</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.283105</td>\n",
              "      <td>0.885496</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.178082</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.251142</td>\n",
              "      <td>0.816794</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.433962</td>\n",
              "      <td>0.324201</td>\n",
              "      <td>0.679389</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.520548</td>\n",
              "      <td>0.702290</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.292237</td>\n",
              "      <td>0.580153</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.433962</td>\n",
              "      <td>0.175799</td>\n",
              "      <td>0.641221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.433962</td>\n",
              "      <td>0.150685</td>\n",
              "      <td>0.587786</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.433962</td>\n",
              "      <td>0.383562</td>\n",
              "      <td>0.625954</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.296804</td>\n",
              "      <td>0.541985</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.312785</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.479167</td>\n",
              "      <td>0.735849</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.694656</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.528302</td>\n",
              "      <td>0.095890</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.395833</td>\n",
              "      <td>0.150943</td>\n",
              "      <td>0.235160</td>\n",
              "      <td>0.740458</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.520833</td>\n",
              "      <td>0.433962</td>\n",
              "      <td>0.257991</td>\n",
              "      <td>0.679389</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.395833</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.340183</td>\n",
              "      <td>0.519084</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.319635</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.729167</td>\n",
              "      <td>0.150943</td>\n",
              "      <td>0.194064</td>\n",
              "      <td>0.557252</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.528302</td>\n",
              "      <td>0.358447</td>\n",
              "      <td>0.694656</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.360731</td>\n",
              "      <td>0.679389</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.223744</td>\n",
              "      <td>0.778626</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.645833</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.182648</td>\n",
              "      <td>0.465649</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.212329</td>\n",
              "      <td>0.664122</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.488584</td>\n",
              "      <td>0.770992</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.770833</td>\n",
              "      <td>0.528302</td>\n",
              "      <td>0.228311</td>\n",
              "      <td>0.328244</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.291667</td>\n",
              "      <td>0.528302</td>\n",
              "      <td>0.276256</td>\n",
              "      <td>0.763359</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.229167</td>\n",
              "      <td>0.150943</td>\n",
              "      <td>0.093607</td>\n",
              "      <td>0.328244</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>0.770833</td>\n",
              "      <td>0.622642</td>\n",
              "      <td>0.232877</td>\n",
              "      <td>0.511450</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>0.354167</td>\n",
              "      <td>0.433962</td>\n",
              "      <td>0.422374</td>\n",
              "      <td>0.374046</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.169811</td>\n",
              "      <td>0.052511</td>\n",
              "      <td>0.412214</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.377358</td>\n",
              "      <td>0.178082</td>\n",
              "      <td>0.694656</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>0.729167</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.230594</td>\n",
              "      <td>0.641221</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>0.770833</td>\n",
              "      <td>0.490566</td>\n",
              "      <td>0.347032</td>\n",
              "      <td>0.618321</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>0.208333</td>\n",
              "      <td>0.415094</td>\n",
              "      <td>0.214612</td>\n",
              "      <td>0.618321</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.566038</td>\n",
              "      <td>0.242009</td>\n",
              "      <td>0.709924</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.162100</td>\n",
              "      <td>0.458015</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.150943</td>\n",
              "      <td>0.477169</td>\n",
              "      <td>0.549618</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.289954</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.320755</td>\n",
              "      <td>0.180365</td>\n",
              "      <td>0.450382</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.264151</td>\n",
              "      <td>0.150685</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.509434</td>\n",
              "      <td>0.175799</td>\n",
              "      <td>0.687023</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.188679</td>\n",
              "      <td>0.438356</td>\n",
              "      <td>0.526718</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.716981</td>\n",
              "      <td>0.226027</td>\n",
              "      <td>0.572519</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.216895</td>\n",
              "      <td>0.702290</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.260274</td>\n",
              "      <td>0.748092</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.547170</td>\n",
              "      <td>0.196347</td>\n",
              "      <td>0.603053</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.358491</td>\n",
              "      <td>0.493151</td>\n",
              "      <td>0.725191</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.098174</td>\n",
              "      <td>0.557252</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.433962</td>\n",
              "      <td>0.139269</td>\n",
              "      <td>0.557252</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.283019</td>\n",
              "      <td>0.162100</td>\n",
              "      <td>0.496183</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.245283</td>\n",
              "      <td>0.070776</td>\n",
              "      <td>0.847328</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>0.114155</td>\n",
              "      <td>0.145038</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.433962</td>\n",
              "      <td>0.262557</td>\n",
              "      <td>0.396947</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.150943</td>\n",
              "      <td>0.315068</td>\n",
              "      <td>0.465649</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.471698</td>\n",
              "      <td>0.152968</td>\n",
              "      <td>0.534351</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.011416</td>\n",
              "      <td>0.335878</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.339623</td>\n",
              "      <td>0.251142</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>297 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          age  trestbps      chol   thalach  ...  oldpeak  slope   ca  class\n",
              "0    0.708333  0.481132  0.244292  0.603053  ...      2.3    3.0  0.0      0\n",
              "1    0.791667  0.622642  0.365297  0.282443  ...      1.5    2.0  3.0      2\n",
              "2    0.791667  0.245283  0.235160  0.442748  ...      2.6    2.0  2.0      1\n",
              "3    0.166667  0.339623  0.283105  0.885496  ...      3.5    3.0  0.0      0\n",
              "4    0.250000  0.339623  0.178082  0.770992  ...      1.4    1.0  0.0      0\n",
              "5    0.562500  0.245283  0.251142  0.816794  ...      0.8    1.0  0.0      0\n",
              "6    0.687500  0.433962  0.324201  0.679389  ...      3.6    3.0  2.0      3\n",
              "7    0.583333  0.245283  0.520548  0.702290  ...      0.6    1.0  0.0      0\n",
              "8    0.708333  0.339623  0.292237  0.580153  ...      1.4    2.0  1.0      2\n",
              "9    0.500000  0.433962  0.175799  0.641221  ...      3.1    3.0  0.0      1\n",
              "10   0.583333  0.433962  0.150685  0.587786  ...      0.4    2.0  0.0      0\n",
              "11   0.562500  0.433962  0.383562  0.625954  ...      1.3    2.0  0.0      0\n",
              "12   0.562500  0.339623  0.296804  0.541985  ...      0.6    2.0  1.0      2\n",
              "13   0.312500  0.245283  0.312785  0.778626  ...      0.0    1.0  0.0      0\n",
              "14   0.479167  0.735849  0.166667  0.694656  ...      0.5    1.0  0.0      0\n",
              "15   0.583333  0.528302  0.095890  0.786260  ...      1.6    1.0  0.0      0\n",
              "16   0.395833  0.150943  0.235160  0.740458  ...      1.0    3.0  0.0      1\n",
              "17   0.520833  0.433962  0.257991  0.679389  ...      1.2    1.0  0.0      0\n",
              "18   0.395833  0.339623  0.340183  0.519084  ...      0.2    1.0  0.0      0\n",
              "19   0.416667  0.339623  0.319635  0.763359  ...      0.6    1.0  0.0      0\n",
              "20   0.729167  0.150943  0.194064  0.557252  ...      1.8    2.0  0.0      0\n",
              "21   0.604167  0.528302  0.358447  0.694656  ...      1.0    1.0  0.0      0\n",
              "22   0.604167  0.245283  0.360731  0.679389  ...      1.8    2.0  0.0      1\n",
              "23   0.604167  0.358491  0.223744  0.778626  ...      3.2    1.0  2.0      3\n",
              "24   0.645833  0.339623  0.182648  0.465649  ...      2.4    2.0  2.0      4\n",
              "25   0.437500  0.245283  0.212329  0.664122  ...      1.6    2.0  0.0      0\n",
              "26   0.604167  0.245283  0.488584  0.770992  ...      0.0    1.0  0.0      0\n",
              "27   0.770833  0.528302  0.228311  0.328244  ...      2.6    3.0  0.0      0\n",
              "28   0.291667  0.528302  0.276256  0.763359  ...      1.5    1.0  0.0      0\n",
              "29   0.229167  0.150943  0.093607  0.328244  ...      2.0    2.0  0.0      3\n",
              "..        ...       ...       ...       ...  ...      ...    ...  ...    ...\n",
              "271  0.770833  0.622642  0.232877  0.511450  ...      2.3    1.0  0.0      0\n",
              "272  0.354167  0.433962  0.422374  0.374046  ...      1.8    2.0  2.0      2\n",
              "273  0.875000  0.169811  0.052511  0.412214  ...      1.6    2.0  0.0      0\n",
              "274  0.625000  0.377358  0.178082  0.694656  ...      0.8    1.0  2.0      1\n",
              "275  0.729167  0.716981  0.230594  0.641221  ...      0.6    2.0  0.0      0\n",
              "276  0.770833  0.490566  0.347032  0.618321  ...      0.0    2.0  1.0      0\n",
              "277  0.208333  0.415094  0.214612  0.618321  ...      0.0    2.0  0.0      0\n",
              "278  0.583333  0.566038  0.242009  0.709924  ...      0.0    1.0  1.0      1\n",
              "279  0.604167  0.339623  0.162100  0.458015  ...      0.6    2.0  0.0      0\n",
              "280  0.583333  0.150943  0.477169  0.549618  ...      3.0    2.0  1.0      2\n",
              "281  0.375000  0.339623  0.289954  0.824427  ...      0.0    1.0  0.0      0\n",
              "282  0.541667  0.320755  0.180365  0.450382  ...      2.0    2.0  1.0      3\n",
              "283  0.125000  0.264151  0.150685  0.786260  ...      0.0    1.0  0.0      0\n",
              "284  0.666667  0.509434  0.175799  0.687023  ...      0.0    1.0  1.0      2\n",
              "285  0.604167  0.188679  0.438356  0.526718  ...      4.4    3.0  3.0      4\n",
              "286  0.604167  0.716981  0.226027  0.572519  ...      2.8    2.0  2.0      2\n",
              "288  0.562500  0.339623  0.216895  0.702290  ...      0.0    1.0  0.0      0\n",
              "289  0.562500  0.245283  0.260274  0.748092  ...      0.0    3.0  0.0      0\n",
              "290  0.791667  0.547170  0.196347  0.603053  ...      0.8    2.0  0.0      1\n",
              "291  0.541667  0.358491  0.493151  0.725191  ...      1.2    1.0  0.0      0\n",
              "292  0.312500  0.245283  0.098174  0.557252  ...      2.8    3.0  0.0      2\n",
              "293  0.708333  0.433962  0.139269  0.557252  ...      4.0    1.0  2.0      2\n",
              "294  0.708333  0.283019  0.162100  0.496183  ...      0.0    2.0  0.0      1\n",
              "295  0.250000  0.245283  0.070776  0.847328  ...      0.0    1.0  0.0      0\n",
              "296  0.625000  0.660377  0.114155  0.145038  ...      1.0    2.0  2.0      3\n",
              "297  0.583333  0.433962  0.262557  0.396947  ...      0.2    2.0  0.0      1\n",
              "298  0.333333  0.150943  0.315068  0.465649  ...      1.2    2.0  0.0      1\n",
              "299  0.812500  0.471698  0.152968  0.534351  ...      3.4    2.0  2.0      2\n",
              "300  0.583333  0.339623  0.011416  0.335878  ...      1.2    2.0  1.0      3\n",
              "301  0.583333  0.339623  0.251142  0.786260  ...      0.0    2.0  1.0      1\n",
              "\n",
              "[297 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVnsgxM06SPr",
        "colab_type": "text"
      },
      "source": [
        "### Now our data is much more uniform but stil represents the results of individual patients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wqkalTW9tWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data_normalized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86Q-qa5QfJyV",
        "colab_type": "text"
      },
      "source": [
        "## Distribution Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djxVDusofcSn",
        "colab_type": "text"
      },
      "source": [
        "#### The dataset is about 300 patients, roughly equally split between patients with heart disease and patients without heart disease. The 'class' column describes the severity of heart disease with class \"4\" being the most severe. The data distribution reflects the rarity of this type of heart disease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBDLrVKoez61",
        "colab_type": "code",
        "outputId": "ccb7edea-dacc-4e50-85ee-a2356cb9adca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        }
      },
      "source": [
        "data.hist(figsize=(12,12))\n",
        "plt.show()"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAK7CAYAAAAA3xInAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x864 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4jrY4fzfFEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "X = np.array(data.drop(['class'], axis=1))\n",
        "y = np.array(data['class'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "014G4mMhhy6s",
        "colab_type": "code",
        "outputId": "093a2eb1-252f-4714-eb97-9cf9843827ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test[:5]"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLHhHvRXiBh6",
        "colab_type": "text"
      },
      "source": [
        "### Change the labels to categoral labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylxkmgGYh7Mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1hjPAUDiNlm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "630eae2d-39e3-455c-d081-56fdd988c110"
      },
      "source": [
        "Y_train = to_categorical(y_train,num_classes=None)\n",
        "Y_test = to_categorical(y_test, num_classes=None)\n",
        "\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)\n",
        "print(X_train.shape)\n",
        "\n",
        "X_train[1]"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(237, 5)\n",
            "(60, 5)\n",
            "(237, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.6875    , 0.43396226, 0.32420091, 0.67938931, 0.        ,\n",
              "       0.        , 4.        , 0.        , 2.        , 0.        ,\n",
              "       3.6       , 3.        , 2.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yYAKpMQpIEe",
        "colab_type": "text"
      },
      "source": [
        "## Define The Neural Network Mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx-v86z5mVwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(units=128, activation=\"relu\", input_shape=(13,)))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation=\"relu\")) \n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=5, activation=\"softmax\"))\n",
        "  \n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(0.0001), metrics=[\"categorical_accuracy\"])\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1stTfE-FJ710",
        "colab_type": "text"
      },
      "source": [
        "## Here's our neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fSap6hdqJXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "99b8b94a-2128-4896-f015-161cd9b4e27c"
      },
      "source": [
        "model = create_model()\n",
        "print(model.summary())"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_82 (Dense)             (None, 128)               1792      \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_63 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 12,293\n",
            "Trainable params: 12,293\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhj-oSHjJ14E",
        "colab_type": "text"
      },
      "source": [
        "## Let's train the model on the patient data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln5E05kzqS_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "469d7132-f54a-40ad-c1e0-2151d6c38faa"
      },
      "source": [
        "model.fit(X_train, Y_train, epochs=300, batch_size=10, verbose=True)"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "237/237 [==============================] - 0s 878us/sample - loss: 1.5828 - categorical_accuracy: 0.2616\n",
            "Epoch 2/300\n",
            "237/237 [==============================] - 0s 163us/sample - loss: 1.5380 - categorical_accuracy: 0.3460\n",
            "Epoch 3/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 1.5226 - categorical_accuracy: 0.3333\n",
            "Epoch 4/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 1.5116 - categorical_accuracy: 0.3966\n",
            "Epoch 5/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 1.4891 - categorical_accuracy: 0.4346\n",
            "Epoch 6/300\n",
            "237/237 [==============================] - 0s 203us/sample - loss: 1.4418 - categorical_accuracy: 0.4726\n",
            "Epoch 7/300\n",
            "237/237 [==============================] - 0s 193us/sample - loss: 1.4565 - categorical_accuracy: 0.4810\n",
            "Epoch 8/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 1.4204 - categorical_accuracy: 0.5148\n",
            "Epoch 9/300\n",
            "237/237 [==============================] - 0s 186us/sample - loss: 1.3670 - categorical_accuracy: 0.5232\n",
            "Epoch 10/300\n",
            "237/237 [==============================] - 0s 191us/sample - loss: 1.3886 - categorical_accuracy: 0.5274\n",
            "Epoch 11/300\n",
            "237/237 [==============================] - 0s 176us/sample - loss: 1.3889 - categorical_accuracy: 0.4937\n",
            "Epoch 12/300\n",
            "237/237 [==============================] - 0s 156us/sample - loss: 1.3829 - categorical_accuracy: 0.5232\n",
            "Epoch 13/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 1.3390 - categorical_accuracy: 0.5570\n",
            "Epoch 14/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 1.3573 - categorical_accuracy: 0.4810\n",
            "Epoch 15/300\n",
            "237/237 [==============================] - 0s 158us/sample - loss: 1.3207 - categorical_accuracy: 0.5359\n",
            "Epoch 16/300\n",
            "237/237 [==============================] - 0s 169us/sample - loss: 1.3031 - categorical_accuracy: 0.5316\n",
            "Epoch 17/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 1.3005 - categorical_accuracy: 0.5274\n",
            "Epoch 18/300\n",
            "237/237 [==============================] - 0s 166us/sample - loss: 1.2799 - categorical_accuracy: 0.5316\n",
            "Epoch 19/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 1.2540 - categorical_accuracy: 0.5316\n",
            "Epoch 20/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 1.2574 - categorical_accuracy: 0.5274\n",
            "Epoch 21/300\n",
            "237/237 [==============================] - 0s 170us/sample - loss: 1.2597 - categorical_accuracy: 0.5190\n",
            "Epoch 22/300\n",
            "237/237 [==============================] - 0s 187us/sample - loss: 1.2707 - categorical_accuracy: 0.5190\n",
            "Epoch 23/300\n",
            "237/237 [==============================] - 0s 169us/sample - loss: 1.2080 - categorical_accuracy: 0.5485\n",
            "Epoch 24/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 1.1993 - categorical_accuracy: 0.5570\n",
            "Epoch 25/300\n",
            "237/237 [==============================] - 0s 167us/sample - loss: 1.2090 - categorical_accuracy: 0.5612\n",
            "Epoch 26/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 1.2185 - categorical_accuracy: 0.5274\n",
            "Epoch 27/300\n",
            "237/237 [==============================] - 0s 196us/sample - loss: 1.1867 - categorical_accuracy: 0.5443\n",
            "Epoch 28/300\n",
            "237/237 [==============================] - 0s 162us/sample - loss: 1.2085 - categorical_accuracy: 0.5401\n",
            "Epoch 29/300\n",
            "237/237 [==============================] - 0s 168us/sample - loss: 1.1964 - categorical_accuracy: 0.5359\n",
            "Epoch 30/300\n",
            "237/237 [==============================] - 0s 169us/sample - loss: 1.1610 - categorical_accuracy: 0.5865\n",
            "Epoch 31/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 1.2102 - categorical_accuracy: 0.5063\n",
            "Epoch 32/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 1.1570 - categorical_accuracy: 0.5696\n",
            "Epoch 33/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 1.1425 - categorical_accuracy: 0.5738\n",
            "Epoch 34/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 1.1612 - categorical_accuracy: 0.5823\n",
            "Epoch 35/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 1.1182 - categorical_accuracy: 0.5738\n",
            "Epoch 36/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 1.0975 - categorical_accuracy: 0.5485\n",
            "Epoch 37/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 1.1227 - categorical_accuracy: 0.5865\n",
            "Epoch 38/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 1.1003 - categorical_accuracy: 0.5443\n",
            "Epoch 39/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 1.0924 - categorical_accuracy: 0.5485\n",
            "Epoch 40/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 1.0714 - categorical_accuracy: 0.5738\n",
            "Epoch 41/300\n",
            "237/237 [==============================] - 0s 182us/sample - loss: 1.0779 - categorical_accuracy: 0.5992\n",
            "Epoch 42/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 1.0672 - categorical_accuracy: 0.5907\n",
            "Epoch 43/300\n",
            "237/237 [==============================] - 0s 189us/sample - loss: 1.0957 - categorical_accuracy: 0.5570\n",
            "Epoch 44/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 1.0671 - categorical_accuracy: 0.5823\n",
            "Epoch 45/300\n",
            "237/237 [==============================] - 0s 182us/sample - loss: 1.0858 - categorical_accuracy: 0.5949\n",
            "Epoch 46/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 1.0593 - categorical_accuracy: 0.6034\n",
            "Epoch 47/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 1.0502 - categorical_accuracy: 0.6118\n",
            "Epoch 48/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 1.0361 - categorical_accuracy: 0.5865\n",
            "Epoch 49/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 1.0678 - categorical_accuracy: 0.6118\n",
            "Epoch 50/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 1.0283 - categorical_accuracy: 0.6160\n",
            "Epoch 51/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 1.0504 - categorical_accuracy: 0.5696\n",
            "Epoch 52/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 1.0069 - categorical_accuracy: 0.6034\n",
            "Epoch 53/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 1.0226 - categorical_accuracy: 0.5865\n",
            "Epoch 54/300\n",
            "237/237 [==============================] - 0s 169us/sample - loss: 1.0079 - categorical_accuracy: 0.5949\n",
            "Epoch 55/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 1.0090 - categorical_accuracy: 0.6034\n",
            "Epoch 56/300\n",
            "237/237 [==============================] - 0s 182us/sample - loss: 1.0333 - categorical_accuracy: 0.6076\n",
            "Epoch 57/300\n",
            "237/237 [==============================] - 0s 165us/sample - loss: 1.0028 - categorical_accuracy: 0.5949\n",
            "Epoch 58/300\n",
            "237/237 [==============================] - 0s 194us/sample - loss: 0.9732 - categorical_accuracy: 0.6287\n",
            "Epoch 59/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.9867 - categorical_accuracy: 0.6076\n",
            "Epoch 60/300\n",
            "237/237 [==============================] - 0s 174us/sample - loss: 0.9911 - categorical_accuracy: 0.5907\n",
            "Epoch 61/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.9875 - categorical_accuracy: 0.5907\n",
            "Epoch 62/300\n",
            "237/237 [==============================] - 0s 165us/sample - loss: 0.9754 - categorical_accuracy: 0.5865\n",
            "Epoch 63/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 0.9714 - categorical_accuracy: 0.6034\n",
            "Epoch 64/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 1.0265 - categorical_accuracy: 0.5823\n",
            "Epoch 65/300\n",
            "237/237 [==============================] - 0s 222us/sample - loss: 0.9851 - categorical_accuracy: 0.5823\n",
            "Epoch 66/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 0.9322 - categorical_accuracy: 0.6498\n",
            "Epoch 67/300\n",
            "237/237 [==============================] - 0s 187us/sample - loss: 1.0182 - categorical_accuracy: 0.5823\n",
            "Epoch 68/300\n",
            "237/237 [==============================] - 0s 188us/sample - loss: 0.9982 - categorical_accuracy: 0.5992\n",
            "Epoch 69/300\n",
            "237/237 [==============================] - 0s 200us/sample - loss: 0.9641 - categorical_accuracy: 0.6160\n",
            "Epoch 70/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.9833 - categorical_accuracy: 0.6118\n",
            "Epoch 71/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.9767 - categorical_accuracy: 0.6160\n",
            "Epoch 72/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.9930 - categorical_accuracy: 0.5949\n",
            "Epoch 73/300\n",
            "237/237 [==============================] - 0s 186us/sample - loss: 0.9928 - categorical_accuracy: 0.5907\n",
            "Epoch 74/300\n",
            "237/237 [==============================] - 0s 186us/sample - loss: 0.9942 - categorical_accuracy: 0.5949\n",
            "Epoch 75/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.9931 - categorical_accuracy: 0.5992\n",
            "Epoch 76/300\n",
            "237/237 [==============================] - 0s 188us/sample - loss: 0.9999 - categorical_accuracy: 0.6034\n",
            "Epoch 77/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.9811 - categorical_accuracy: 0.5865\n",
            "Epoch 78/300\n",
            "237/237 [==============================] - 0s 189us/sample - loss: 0.9571 - categorical_accuracy: 0.6245\n",
            "Epoch 79/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 0.9754 - categorical_accuracy: 0.5992\n",
            "Epoch 80/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.9566 - categorical_accuracy: 0.6203\n",
            "Epoch 81/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 0.9662 - categorical_accuracy: 0.5949\n",
            "Epoch 82/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.9496 - categorical_accuracy: 0.5907\n",
            "Epoch 83/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.9401 - categorical_accuracy: 0.6034\n",
            "Epoch 84/300\n",
            "237/237 [==============================] - 0s 163us/sample - loss: 0.9869 - categorical_accuracy: 0.6034\n",
            "Epoch 85/300\n",
            "237/237 [==============================] - 0s 194us/sample - loss: 0.9571 - categorical_accuracy: 0.6245\n",
            "Epoch 86/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 1.0132 - categorical_accuracy: 0.5823\n",
            "Epoch 87/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.9343 - categorical_accuracy: 0.6118\n",
            "Epoch 88/300\n",
            "237/237 [==============================] - 0s 167us/sample - loss: 0.9330 - categorical_accuracy: 0.5949\n",
            "Epoch 89/300\n",
            "237/237 [==============================] - 0s 170us/sample - loss: 0.9516 - categorical_accuracy: 0.5907\n",
            "Epoch 90/300\n",
            "237/237 [==============================] - 0s 176us/sample - loss: 0.9595 - categorical_accuracy: 0.6414\n",
            "Epoch 91/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 0.8984 - categorical_accuracy: 0.6287\n",
            "Epoch 92/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.9654 - categorical_accuracy: 0.6245\n",
            "Epoch 93/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 0.9428 - categorical_accuracy: 0.6287\n",
            "Epoch 94/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.9332 - categorical_accuracy: 0.6245\n",
            "Epoch 95/300\n",
            "237/237 [==============================] - 0s 170us/sample - loss: 0.9314 - categorical_accuracy: 0.6245\n",
            "Epoch 96/300\n",
            "237/237 [==============================] - 0s 167us/sample - loss: 0.9408 - categorical_accuracy: 0.6203\n",
            "Epoch 97/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 0.9592 - categorical_accuracy: 0.6203\n",
            "Epoch 98/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.9841 - categorical_accuracy: 0.6456\n",
            "Epoch 99/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.9260 - categorical_accuracy: 0.6203\n",
            "Epoch 100/300\n",
            "237/237 [==============================] - 0s 186us/sample - loss: 0.9775 - categorical_accuracy: 0.5907\n",
            "Epoch 101/300\n",
            "237/237 [==============================] - 0s 187us/sample - loss: 0.9225 - categorical_accuracy: 0.6076\n",
            "Epoch 102/300\n",
            "237/237 [==============================] - 0s 174us/sample - loss: 0.9301 - categorical_accuracy: 0.5907\n",
            "Epoch 103/300\n",
            "237/237 [==============================] - 0s 203us/sample - loss: 0.9258 - categorical_accuracy: 0.5949\n",
            "Epoch 104/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.9293 - categorical_accuracy: 0.6245\n",
            "Epoch 105/300\n",
            "237/237 [==============================] - 0s 160us/sample - loss: 0.9213 - categorical_accuracy: 0.6245\n",
            "Epoch 106/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.9198 - categorical_accuracy: 0.6118\n",
            "Epoch 107/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.9191 - categorical_accuracy: 0.6160\n",
            "Epoch 108/300\n",
            "237/237 [==============================] - 0s 188us/sample - loss: 0.9640 - categorical_accuracy: 0.5865\n",
            "Epoch 109/300\n",
            "237/237 [==============================] - 0s 158us/sample - loss: 0.9575 - categorical_accuracy: 0.5865\n",
            "Epoch 110/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 0.8952 - categorical_accuracy: 0.6582\n",
            "Epoch 111/300\n",
            "237/237 [==============================] - 0s 174us/sample - loss: 0.9348 - categorical_accuracy: 0.5949\n",
            "Epoch 112/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.9193 - categorical_accuracy: 0.6245\n",
            "Epoch 113/300\n",
            "237/237 [==============================] - 0s 161us/sample - loss: 0.8543 - categorical_accuracy: 0.6835\n",
            "Epoch 114/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.8953 - categorical_accuracy: 0.6456\n",
            "Epoch 115/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.9258 - categorical_accuracy: 0.5992\n",
            "Epoch 116/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 0.9203 - categorical_accuracy: 0.6371\n",
            "Epoch 117/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 0.9108 - categorical_accuracy: 0.6034\n",
            "Epoch 118/300\n",
            "237/237 [==============================] - 0s 169us/sample - loss: 0.9063 - categorical_accuracy: 0.6287\n",
            "Epoch 119/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.9147 - categorical_accuracy: 0.6245\n",
            "Epoch 120/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.9154 - categorical_accuracy: 0.6245\n",
            "Epoch 121/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.8936 - categorical_accuracy: 0.6498\n",
            "Epoch 122/300\n",
            "237/237 [==============================] - 0s 157us/sample - loss: 0.9282 - categorical_accuracy: 0.5865\n",
            "Epoch 123/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.9018 - categorical_accuracy: 0.6118\n",
            "Epoch 124/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.8998 - categorical_accuracy: 0.6076\n",
            "Epoch 125/300\n",
            "237/237 [==============================] - 0s 168us/sample - loss: 0.8840 - categorical_accuracy: 0.6329\n",
            "Epoch 126/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.9156 - categorical_accuracy: 0.6076\n",
            "Epoch 127/300\n",
            "237/237 [==============================] - 0s 196us/sample - loss: 0.9163 - categorical_accuracy: 0.6329\n",
            "Epoch 128/300\n",
            "237/237 [==============================] - 0s 182us/sample - loss: 0.9098 - categorical_accuracy: 0.6329\n",
            "Epoch 129/300\n",
            "237/237 [==============================] - 0s 170us/sample - loss: 0.8813 - categorical_accuracy: 0.6287\n",
            "Epoch 130/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.8988 - categorical_accuracy: 0.6329\n",
            "Epoch 131/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 0.9212 - categorical_accuracy: 0.6245\n",
            "Epoch 132/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 0.9000 - categorical_accuracy: 0.6203\n",
            "Epoch 133/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.9144 - categorical_accuracy: 0.6203\n",
            "Epoch 134/300\n",
            "237/237 [==============================] - 0s 186us/sample - loss: 0.9107 - categorical_accuracy: 0.6160\n",
            "Epoch 135/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.8844 - categorical_accuracy: 0.6456\n",
            "Epoch 136/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.8500 - categorical_accuracy: 0.6287\n",
            "Epoch 137/300\n",
            "237/237 [==============================] - 0s 196us/sample - loss: 0.8975 - categorical_accuracy: 0.6371\n",
            "Epoch 138/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.8721 - categorical_accuracy: 0.6371\n",
            "Epoch 139/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.9117 - categorical_accuracy: 0.6329\n",
            "Epoch 140/300\n",
            "237/237 [==============================] - 0s 166us/sample - loss: 0.8976 - categorical_accuracy: 0.6456\n",
            "Epoch 141/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.8939 - categorical_accuracy: 0.6498\n",
            "Epoch 142/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 0.9028 - categorical_accuracy: 0.6287\n",
            "Epoch 143/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 0.8489 - categorical_accuracy: 0.6245\n",
            "Epoch 144/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 0.8788 - categorical_accuracy: 0.6414\n",
            "Epoch 145/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.9040 - categorical_accuracy: 0.6329\n",
            "Epoch 146/300\n",
            "237/237 [==============================] - 0s 182us/sample - loss: 0.8824 - categorical_accuracy: 0.6371\n",
            "Epoch 147/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.8987 - categorical_accuracy: 0.6203\n",
            "Epoch 148/300\n",
            "237/237 [==============================] - 0s 176us/sample - loss: 0.9105 - categorical_accuracy: 0.6076\n",
            "Epoch 149/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.8858 - categorical_accuracy: 0.5907\n",
            "Epoch 150/300\n",
            "237/237 [==============================] - 0s 169us/sample - loss: 0.8984 - categorical_accuracy: 0.6203\n",
            "Epoch 151/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.8780 - categorical_accuracy: 0.6414\n",
            "Epoch 152/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.8625 - categorical_accuracy: 0.6118\n",
            "Epoch 153/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.8463 - categorical_accuracy: 0.6329\n",
            "Epoch 154/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 0.9107 - categorical_accuracy: 0.6203\n",
            "Epoch 155/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.8780 - categorical_accuracy: 0.6203\n",
            "Epoch 156/300\n",
            "237/237 [==============================] - 0s 187us/sample - loss: 0.8703 - categorical_accuracy: 0.6371\n",
            "Epoch 157/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 0.8658 - categorical_accuracy: 0.6287\n",
            "Epoch 158/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 0.8588 - categorical_accuracy: 0.6160\n",
            "Epoch 159/300\n",
            "237/237 [==============================] - 0s 191us/sample - loss: 0.8437 - categorical_accuracy: 0.6498\n",
            "Epoch 160/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 0.8779 - categorical_accuracy: 0.6582\n",
            "Epoch 161/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.8610 - categorical_accuracy: 0.6582\n",
            "Epoch 162/300\n",
            "237/237 [==============================] - 0s 191us/sample - loss: 0.8768 - categorical_accuracy: 0.6540\n",
            "Epoch 163/300\n",
            "237/237 [==============================] - 0s 184us/sample - loss: 0.8746 - categorical_accuracy: 0.6371\n",
            "Epoch 164/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.8729 - categorical_accuracy: 0.6582\n",
            "Epoch 165/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 0.8791 - categorical_accuracy: 0.6540\n",
            "Epoch 166/300\n",
            "237/237 [==============================] - 0s 166us/sample - loss: 0.8712 - categorical_accuracy: 0.6414\n",
            "Epoch 167/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.8904 - categorical_accuracy: 0.6414\n",
            "Epoch 168/300\n",
            "237/237 [==============================] - 0s 166us/sample - loss: 0.8531 - categorical_accuracy: 0.6329\n",
            "Epoch 169/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.8668 - categorical_accuracy: 0.6245\n",
            "Epoch 170/300\n",
            "237/237 [==============================] - 0s 188us/sample - loss: 0.8674 - categorical_accuracy: 0.6414\n",
            "Epoch 171/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.8581 - categorical_accuracy: 0.6835\n",
            "Epoch 172/300\n",
            "237/237 [==============================] - 0s 182us/sample - loss: 0.8542 - categorical_accuracy: 0.6329\n",
            "Epoch 173/300\n",
            "237/237 [==============================] - 0s 169us/sample - loss: 0.8494 - categorical_accuracy: 0.6203\n",
            "Epoch 174/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 0.8870 - categorical_accuracy: 0.6456\n",
            "Epoch 175/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.8742 - categorical_accuracy: 0.6540\n",
            "Epoch 176/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 0.8666 - categorical_accuracy: 0.6245\n",
            "Epoch 177/300\n",
            "237/237 [==============================] - 0s 187us/sample - loss: 0.8573 - categorical_accuracy: 0.6287\n",
            "Epoch 178/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 0.8306 - categorical_accuracy: 0.6582\n",
            "Epoch 179/300\n",
            "237/237 [==============================] - 0s 184us/sample - loss: 0.8645 - categorical_accuracy: 0.6582\n",
            "Epoch 180/300\n",
            "237/237 [==============================] - 0s 176us/sample - loss: 0.8432 - categorical_accuracy: 0.6624\n",
            "Epoch 181/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.8481 - categorical_accuracy: 0.6498\n",
            "Epoch 182/300\n",
            "237/237 [==============================] - 0s 191us/sample - loss: 0.8432 - categorical_accuracy: 0.6920\n",
            "Epoch 183/300\n",
            "237/237 [==============================] - 0s 193us/sample - loss: 0.8733 - categorical_accuracy: 0.6624\n",
            "Epoch 184/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 0.8470 - categorical_accuracy: 0.6751\n",
            "Epoch 185/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 0.8546 - categorical_accuracy: 0.6329\n",
            "Epoch 186/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 0.8638 - categorical_accuracy: 0.6414\n",
            "Epoch 187/300\n",
            "237/237 [==============================] - 0s 165us/sample - loss: 0.8768 - categorical_accuracy: 0.6329\n",
            "Epoch 188/300\n",
            "237/237 [==============================] - 0s 168us/sample - loss: 0.8382 - categorical_accuracy: 0.6667\n",
            "Epoch 189/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 0.8200 - categorical_accuracy: 0.6582\n",
            "Epoch 190/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.8656 - categorical_accuracy: 0.6371\n",
            "Epoch 191/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.8263 - categorical_accuracy: 0.6667\n",
            "Epoch 192/300\n",
            "237/237 [==============================] - 0s 166us/sample - loss: 0.8922 - categorical_accuracy: 0.6245\n",
            "Epoch 193/300\n",
            "237/237 [==============================] - 0s 168us/sample - loss: 0.8726 - categorical_accuracy: 0.6498\n",
            "Epoch 194/300\n",
            "237/237 [==============================] - 0s 184us/sample - loss: 0.8334 - categorical_accuracy: 0.6540\n",
            "Epoch 195/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.8557 - categorical_accuracy: 0.6076\n",
            "Epoch 196/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 0.8245 - categorical_accuracy: 0.6624\n",
            "Epoch 197/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.8565 - categorical_accuracy: 0.6540\n",
            "Epoch 198/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.8627 - categorical_accuracy: 0.6287\n",
            "Epoch 199/300\n",
            "237/237 [==============================] - 0s 191us/sample - loss: 0.8330 - categorical_accuracy: 0.6751\n",
            "Epoch 200/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.8275 - categorical_accuracy: 0.6751\n",
            "Epoch 201/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.8209 - categorical_accuracy: 0.6498\n",
            "Epoch 202/300\n",
            "237/237 [==============================] - 0s 289us/sample - loss: 0.8319 - categorical_accuracy: 0.6371\n",
            "Epoch 203/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.8503 - categorical_accuracy: 0.6456\n",
            "Epoch 204/300\n",
            "237/237 [==============================] - 0s 167us/sample - loss: 0.7920 - categorical_accuracy: 0.6920\n",
            "Epoch 205/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 0.8408 - categorical_accuracy: 0.6498\n",
            "Epoch 206/300\n",
            "237/237 [==============================] - 0s 161us/sample - loss: 0.8411 - categorical_accuracy: 0.6456\n",
            "Epoch 207/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.8356 - categorical_accuracy: 0.6456\n",
            "Epoch 208/300\n",
            "237/237 [==============================] - 0s 159us/sample - loss: 0.8082 - categorical_accuracy: 0.6878\n",
            "Epoch 209/300\n",
            "237/237 [==============================] - 0s 189us/sample - loss: 0.8106 - categorical_accuracy: 0.6709\n",
            "Epoch 210/300\n",
            "237/237 [==============================] - 0s 162us/sample - loss: 0.8345 - categorical_accuracy: 0.6835\n",
            "Epoch 211/300\n",
            "237/237 [==============================] - 0s 166us/sample - loss: 0.8376 - categorical_accuracy: 0.6624\n",
            "Epoch 212/300\n",
            "237/237 [==============================] - 0s 187us/sample - loss: 0.8061 - categorical_accuracy: 0.6582\n",
            "Epoch 213/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 0.8254 - categorical_accuracy: 0.6667\n",
            "Epoch 214/300\n",
            "237/237 [==============================] - 0s 158us/sample - loss: 0.8297 - categorical_accuracy: 0.6329\n",
            "Epoch 215/300\n",
            "237/237 [==============================] - 0s 187us/sample - loss: 0.8187 - categorical_accuracy: 0.6498\n",
            "Epoch 216/300\n",
            "237/237 [==============================] - 0s 191us/sample - loss: 0.8228 - categorical_accuracy: 0.6709\n",
            "Epoch 217/300\n",
            "237/237 [==============================] - 0s 176us/sample - loss: 0.8141 - categorical_accuracy: 0.6667\n",
            "Epoch 218/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.8342 - categorical_accuracy: 0.6793\n",
            "Epoch 219/300\n",
            "237/237 [==============================] - 0s 163us/sample - loss: 0.8177 - categorical_accuracy: 0.6540\n",
            "Epoch 220/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.8010 - categorical_accuracy: 0.6667\n",
            "Epoch 221/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.8121 - categorical_accuracy: 0.6878\n",
            "Epoch 222/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.8279 - categorical_accuracy: 0.6582\n",
            "Epoch 223/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.7986 - categorical_accuracy: 0.6667\n",
            "Epoch 224/300\n",
            "237/237 [==============================] - 0s 176us/sample - loss: 0.7995 - categorical_accuracy: 0.7046\n",
            "Epoch 225/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 0.8051 - categorical_accuracy: 0.6624\n",
            "Epoch 226/300\n",
            "237/237 [==============================] - 0s 163us/sample - loss: 0.8080 - categorical_accuracy: 0.6329\n",
            "Epoch 227/300\n",
            "237/237 [==============================] - 0s 164us/sample - loss: 0.8228 - categorical_accuracy: 0.6624\n",
            "Epoch 228/300\n",
            "237/237 [==============================] - 0s 174us/sample - loss: 0.8419 - categorical_accuracy: 0.6709\n",
            "Epoch 229/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 0.8323 - categorical_accuracy: 0.6624\n",
            "Epoch 230/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.8066 - categorical_accuracy: 0.6624\n",
            "Epoch 231/300\n",
            "237/237 [==============================] - 0s 168us/sample - loss: 0.7798 - categorical_accuracy: 0.6878\n",
            "Epoch 232/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 0.7986 - categorical_accuracy: 0.6962\n",
            "Epoch 233/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.7787 - categorical_accuracy: 0.6624\n",
            "Epoch 234/300\n",
            "237/237 [==============================] - 0s 184us/sample - loss: 0.8132 - categorical_accuracy: 0.6498\n",
            "Epoch 235/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.7990 - categorical_accuracy: 0.6667\n",
            "Epoch 236/300\n",
            "237/237 [==============================] - 0s 187us/sample - loss: 0.8139 - categorical_accuracy: 0.6498\n",
            "Epoch 237/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.7863 - categorical_accuracy: 0.6920\n",
            "Epoch 238/300\n",
            "237/237 [==============================] - 0s 184us/sample - loss: 0.7944 - categorical_accuracy: 0.6709\n",
            "Epoch 239/300\n",
            "237/237 [==============================] - 0s 166us/sample - loss: 0.8173 - categorical_accuracy: 0.6582\n",
            "Epoch 240/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.7841 - categorical_accuracy: 0.6414\n",
            "Epoch 241/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.7835 - categorical_accuracy: 0.6624\n",
            "Epoch 242/300\n",
            "237/237 [==============================] - 0s 191us/sample - loss: 0.7994 - categorical_accuracy: 0.6835\n",
            "Epoch 243/300\n",
            "237/237 [==============================] - 0s 160us/sample - loss: 0.7946 - categorical_accuracy: 0.6667\n",
            "Epoch 244/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.8007 - categorical_accuracy: 0.6751\n",
            "Epoch 245/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 0.7973 - categorical_accuracy: 0.6456\n",
            "Epoch 246/300\n",
            "237/237 [==============================] - 0s 162us/sample - loss: 0.7987 - categorical_accuracy: 0.6624\n",
            "Epoch 247/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.7824 - categorical_accuracy: 0.6624\n",
            "Epoch 248/300\n",
            "237/237 [==============================] - 0s 165us/sample - loss: 0.7961 - categorical_accuracy: 0.6540\n",
            "Epoch 249/300\n",
            "237/237 [==============================] - 0s 194us/sample - loss: 0.8115 - categorical_accuracy: 0.6667\n",
            "Epoch 250/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.8182 - categorical_accuracy: 0.6582\n",
            "Epoch 251/300\n",
            "237/237 [==============================] - 0s 163us/sample - loss: 0.7873 - categorical_accuracy: 0.6709\n",
            "Epoch 252/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 0.7753 - categorical_accuracy: 0.6582\n",
            "Epoch 253/300\n",
            "237/237 [==============================] - 0s 169us/sample - loss: 0.7682 - categorical_accuracy: 0.6624\n",
            "Epoch 254/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 0.7866 - categorical_accuracy: 0.6835\n",
            "Epoch 255/300\n",
            "237/237 [==============================] - 0s 167us/sample - loss: 0.7816 - categorical_accuracy: 0.6414\n",
            "Epoch 256/300\n",
            "237/237 [==============================] - 0s 174us/sample - loss: 0.7802 - categorical_accuracy: 0.6456\n",
            "Epoch 257/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 0.7989 - categorical_accuracy: 0.6793\n",
            "Epoch 258/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 0.7836 - categorical_accuracy: 0.6751\n",
            "Epoch 259/300\n",
            "237/237 [==============================] - 0s 165us/sample - loss: 0.7507 - categorical_accuracy: 0.6835\n",
            "Epoch 260/300\n",
            "237/237 [==============================] - 0s 157us/sample - loss: 0.7572 - categorical_accuracy: 0.7004\n",
            "Epoch 261/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 0.7878 - categorical_accuracy: 0.6498\n",
            "Epoch 262/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.7932 - categorical_accuracy: 0.6582\n",
            "Epoch 263/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.7675 - categorical_accuracy: 0.6835\n",
            "Epoch 264/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.8129 - categorical_accuracy: 0.6709\n",
            "Epoch 265/300\n",
            "237/237 [==============================] - 0s 184us/sample - loss: 0.7473 - categorical_accuracy: 0.7046\n",
            "Epoch 266/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.7700 - categorical_accuracy: 0.6878\n",
            "Epoch 267/300\n",
            "237/237 [==============================] - 0s 183us/sample - loss: 0.7833 - categorical_accuracy: 0.6582\n",
            "Epoch 268/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 0.7945 - categorical_accuracy: 0.6456\n",
            "Epoch 269/300\n",
            "237/237 [==============================] - 0s 173us/sample - loss: 0.8213 - categorical_accuracy: 0.6329\n",
            "Epoch 270/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.7876 - categorical_accuracy: 0.6287\n",
            "Epoch 271/300\n",
            "237/237 [==============================] - 0s 170us/sample - loss: 0.7768 - categorical_accuracy: 0.6709\n",
            "Epoch 272/300\n",
            "237/237 [==============================] - 0s 172us/sample - loss: 0.7528 - categorical_accuracy: 0.6793\n",
            "Epoch 273/300\n",
            "237/237 [==============================] - 0s 165us/sample - loss: 0.7799 - categorical_accuracy: 0.6667\n",
            "Epoch 274/300\n",
            "237/237 [==============================] - 0s 162us/sample - loss: 0.8105 - categorical_accuracy: 0.6582\n",
            "Epoch 275/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.7713 - categorical_accuracy: 0.6667\n",
            "Epoch 276/300\n",
            "237/237 [==============================] - 0s 191us/sample - loss: 0.7779 - categorical_accuracy: 0.7004\n",
            "Epoch 277/300\n",
            "237/237 [==============================] - 0s 167us/sample - loss: 0.7645 - categorical_accuracy: 0.6582\n",
            "Epoch 278/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.7546 - categorical_accuracy: 0.6793\n",
            "Epoch 279/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 0.7650 - categorical_accuracy: 0.6667\n",
            "Epoch 280/300\n",
            "237/237 [==============================] - 0s 185us/sample - loss: 0.7533 - categorical_accuracy: 0.6878\n",
            "Epoch 281/300\n",
            "237/237 [==============================] - 0s 167us/sample - loss: 0.8007 - categorical_accuracy: 0.6498\n",
            "Epoch 282/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.7900 - categorical_accuracy: 0.6793\n",
            "Epoch 283/300\n",
            "237/237 [==============================] - 0s 181us/sample - loss: 0.7570 - categorical_accuracy: 0.6582\n",
            "Epoch 284/300\n",
            "237/237 [==============================] - 0s 176us/sample - loss: 0.7519 - categorical_accuracy: 0.7342\n",
            "Epoch 285/300\n",
            "237/237 [==============================] - 0s 179us/sample - loss: 0.7616 - categorical_accuracy: 0.6920\n",
            "Epoch 286/300\n",
            "237/237 [==============================] - 0s 162us/sample - loss: 0.7427 - categorical_accuracy: 0.7046\n",
            "Epoch 287/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.7643 - categorical_accuracy: 0.6793\n",
            "Epoch 288/300\n",
            "237/237 [==============================] - 0s 174us/sample - loss: 0.7766 - categorical_accuracy: 0.6878\n",
            "Epoch 289/300\n",
            "237/237 [==============================] - 0s 176us/sample - loss: 0.7643 - categorical_accuracy: 0.6835\n",
            "Epoch 290/300\n",
            "237/237 [==============================] - 0s 177us/sample - loss: 0.7592 - categorical_accuracy: 0.6793\n",
            "Epoch 291/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 0.7568 - categorical_accuracy: 0.6709\n",
            "Epoch 292/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.7481 - categorical_accuracy: 0.6878\n",
            "Epoch 293/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.7804 - categorical_accuracy: 0.6878\n",
            "Epoch 294/300\n",
            "237/237 [==============================] - 0s 178us/sample - loss: 0.7552 - categorical_accuracy: 0.6835\n",
            "Epoch 295/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.7823 - categorical_accuracy: 0.6878\n",
            "Epoch 296/300\n",
            "237/237 [==============================] - 0s 184us/sample - loss: 0.7308 - categorical_accuracy: 0.6878\n",
            "Epoch 297/300\n",
            "237/237 [==============================] - 0s 182us/sample - loss: 0.7448 - categorical_accuracy: 0.6878\n",
            "Epoch 298/300\n",
            "237/237 [==============================] - 0s 175us/sample - loss: 0.7507 - categorical_accuracy: 0.6920\n",
            "Epoch 299/300\n",
            "237/237 [==============================] - 0s 194us/sample - loss: 0.7396 - categorical_accuracy: 0.6667\n",
            "Epoch 300/300\n",
            "237/237 [==============================] - 0s 171us/sample - loss: 0.7207 - categorical_accuracy: 0.7004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f736530eb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LisNKixEKBKV",
        "colab_type": "text"
      },
      "source": [
        "## About 70% accuracy on the training data for predicting the type of heart disease...whomp whomp..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58KPUMehxKiA",
        "colab_type": "text"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tDUpE38xQel",
        "colab_type": "text"
      },
      "source": [
        "###Due to the size of our dataset and the limited amount of featuers we're using, our machine learning model is definitely better than a random guess, however as a diagnostic tool, its far from state of the art at 70%. This may be because we need more features included in our dataset to accurately diagnore the type of heart disease, but also may be related to the fact that our dataset is quite small (only 300 patients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY96pdxxlEN",
        "colab_type": "text"
      },
      "source": [
        "### Let's see what happens if we convert this to a binary classification problem...in other words, rather than trying to predict the specific type of heart disease a patient may have, we switch the prediction to \"Does the patient have heart disease: yes or no?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qkCwD4cx487",
        "colab_type": "text"
      },
      "source": [
        "#### Make a copy of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlJpnMT2sqfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test_binary = y_test.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrIOJRGhqcgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_binary = y_train.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeDG8hNRyAXy",
        "colab_type": "text"
      },
      "source": [
        "#### We need to change our training and tests labels to just 0's or 1's...0 a patient does not have heart disease, 1 a patient does have heart disease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO2QD8U3swf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_binary[Y_train_binary > 0] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMLU9CbRs_2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test_binary[Y_test_binary > 0] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj6JvT00tGEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2999bb9-7f14-4aa6-ea06-1c36b8081af3"
      },
      "source": [
        "print(Y_test_binary[:20])"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ0EGEAAyNXm",
        "colab_type": "text"
      },
      "source": [
        "### Build a binary classification model. Here I'm using almost the same neural network architecture, but the output layer changes to only a single neuron with a sigmoid activation function for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzcy17-ItIQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_binary_model():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(units=128, activation=\"relu\", input_shape=(13,)))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation=\"relu\")) \n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation=\"relu\")) \n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(units=8, activation=\"relu\")) \n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "  \n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(0.001), metrics=[\"accuracy\"])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwDFq6BuyaSE",
        "colab_type": "text"
      },
      "source": [
        "### Build and summarize the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztd5HVRlti_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "ec851112-8355-4eb2-d5d4-5dc5322b9311"
      },
      "source": [
        "model_binary = create_binary_model()\n",
        "print(model_binary.summary())"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_86 (Dense)             (None, 128)               1792      \n",
            "_________________________________________________________________\n",
            "dropout_64 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_65 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_66 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 12,801\n",
            "Trainable params: 12,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY54ysEyydHX",
        "colab_type": "text"
      },
      "source": [
        "### Training our new model, accuracy is definitely better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg-Js78Wtmas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a30d298d-fec0-496d-b9e8-091d0ce2b8b5"
      },
      "source": [
        "model_binary.fit(X_train, Y_train_binary, epochs=300, batch_size=10, verbose=1)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "237/237 [==============================] - 0s 1ms/sample - loss: 0.7858 - accuracy: 0.4641\n",
            "Epoch 2/300\n",
            "237/237 [==============================] - 0s 184us/sample - loss: 0.7606 - accuracy: 0.4219\n",
            "Epoch 3/300\n",
            "237/237 [==============================] - 0s 184us/sample - loss: 0.7281 - accuracy: 0.4895\n",
            "Epoch 4/300\n",
            "237/237 [==============================] - 0s 180us/sample - loss: 0.7252 - accuracy: 0.4557\n",
            "Epoch 5/300\n",
            "237/237 [==============================] - 0s 194us/sample - loss: 0.7078 - accuracy: 0.4979\n",
            "Epoch 6/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.7140 - accuracy: 0.4430\n",
            "Epoch 7/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.7033 - accuracy: 0.4810\n",
            "Epoch 8/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 0.7075 - accuracy: 0.4430\n",
            "Epoch 9/300\n",
            "237/237 [==============================] - 0s 258us/sample - loss: 0.7072 - accuracy: 0.4937\n",
            "Epoch 10/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.6996 - accuracy: 0.5316\n",
            "Epoch 11/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.7049 - accuracy: 0.4515\n",
            "Epoch 12/300\n",
            "237/237 [==============================] - 0s 194us/sample - loss: 0.6923 - accuracy: 0.5316\n",
            "Epoch 13/300\n",
            "237/237 [==============================] - 0s 212us/sample - loss: 0.6987 - accuracy: 0.5274\n",
            "Epoch 14/300\n",
            "237/237 [==============================] - 0s 200us/sample - loss: 0.7008 - accuracy: 0.5190\n",
            "Epoch 15/300\n",
            "237/237 [==============================] - 0s 224us/sample - loss: 0.7008 - accuracy: 0.5063\n",
            "Epoch 16/300\n",
            "237/237 [==============================] - 0s 198us/sample - loss: 0.6850 - accuracy: 0.5485\n",
            "Epoch 17/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.6932 - accuracy: 0.4895\n",
            "Epoch 18/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.6899 - accuracy: 0.5148\n",
            "Epoch 19/300\n",
            "237/237 [==============================] - 0s 212us/sample - loss: 0.6988 - accuracy: 0.5148\n",
            "Epoch 20/300\n",
            "237/237 [==============================] - 0s 220us/sample - loss: 0.6936 - accuracy: 0.5274\n",
            "Epoch 21/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.6851 - accuracy: 0.5063\n",
            "Epoch 22/300\n",
            "237/237 [==============================] - 0s 215us/sample - loss: 0.6897 - accuracy: 0.5274\n",
            "Epoch 23/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.7037 - accuracy: 0.5274\n",
            "Epoch 24/300\n",
            "237/237 [==============================] - 0s 224us/sample - loss: 0.6910 - accuracy: 0.5696\n",
            "Epoch 25/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.6811 - accuracy: 0.5781\n",
            "Epoch 26/300\n",
            "237/237 [==============================] - 0s 226us/sample - loss: 0.6806 - accuracy: 0.5992\n",
            "Epoch 27/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.6777 - accuracy: 0.5612\n",
            "Epoch 28/300\n",
            "237/237 [==============================] - 0s 203us/sample - loss: 0.6882 - accuracy: 0.5232\n",
            "Epoch 29/300\n",
            "237/237 [==============================] - 0s 234us/sample - loss: 0.6766 - accuracy: 0.5612\n",
            "Epoch 30/300\n",
            "237/237 [==============================] - 0s 227us/sample - loss: 0.6780 - accuracy: 0.5823\n",
            "Epoch 31/300\n",
            "237/237 [==============================] - 0s 238us/sample - loss: 0.6552 - accuracy: 0.6245\n",
            "Epoch 32/300\n",
            "237/237 [==============================] - 0s 216us/sample - loss: 0.6603 - accuracy: 0.5738\n",
            "Epoch 33/300\n",
            "237/237 [==============================] - 0s 212us/sample - loss: 0.6653 - accuracy: 0.5781\n",
            "Epoch 34/300\n",
            "237/237 [==============================] - 0s 219us/sample - loss: 0.6680 - accuracy: 0.6160\n",
            "Epoch 35/300\n",
            "237/237 [==============================] - 0s 230us/sample - loss: 0.6548 - accuracy: 0.6498\n",
            "Epoch 36/300\n",
            "237/237 [==============================] - 0s 203us/sample - loss: 0.6546 - accuracy: 0.6624\n",
            "Epoch 37/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.6165 - accuracy: 0.6878\n",
            "Epoch 38/300\n",
            "237/237 [==============================] - 0s 227us/sample - loss: 0.6111 - accuracy: 0.6793\n",
            "Epoch 39/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.6476 - accuracy: 0.6624\n",
            "Epoch 40/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.6282 - accuracy: 0.6751\n",
            "Epoch 41/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.6203 - accuracy: 0.6751\n",
            "Epoch 42/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.6194 - accuracy: 0.6793\n",
            "Epoch 43/300\n",
            "237/237 [==============================] - 0s 234us/sample - loss: 0.6569 - accuracy: 0.6203\n",
            "Epoch 44/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.5870 - accuracy: 0.7173\n",
            "Epoch 45/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.6093 - accuracy: 0.6878\n",
            "Epoch 46/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.5954 - accuracy: 0.6835\n",
            "Epoch 47/300\n",
            "237/237 [==============================] - 0s 212us/sample - loss: 0.5704 - accuracy: 0.7511\n",
            "Epoch 48/300\n",
            "237/237 [==============================] - 0s 206us/sample - loss: 0.5677 - accuracy: 0.7679\n",
            "Epoch 49/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.5669 - accuracy: 0.7300\n",
            "Epoch 50/300\n",
            "237/237 [==============================] - 0s 200us/sample - loss: 0.5173 - accuracy: 0.7679\n",
            "Epoch 51/300\n",
            "237/237 [==============================] - 0s 234us/sample - loss: 0.5487 - accuracy: 0.7257\n",
            "Epoch 52/300\n",
            "237/237 [==============================] - 0s 222us/sample - loss: 0.5869 - accuracy: 0.7595\n",
            "Epoch 53/300\n",
            "237/237 [==============================] - 0s 220us/sample - loss: 0.5499 - accuracy: 0.7975\n",
            "Epoch 54/300\n",
            "237/237 [==============================] - 0s 214us/sample - loss: 0.5758 - accuracy: 0.6962\n",
            "Epoch 55/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.5547 - accuracy: 0.7173\n",
            "Epoch 56/300\n",
            "237/237 [==============================] - 0s 226us/sample - loss: 0.5416 - accuracy: 0.7637\n",
            "Epoch 57/300\n",
            "237/237 [==============================] - 0s 218us/sample - loss: 0.6409 - accuracy: 0.7426\n",
            "Epoch 58/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.5172 - accuracy: 0.8143\n",
            "Epoch 59/300\n",
            "237/237 [==============================] - 0s 197us/sample - loss: 0.5588 - accuracy: 0.7215\n",
            "Epoch 60/300\n",
            "237/237 [==============================] - 0s 208us/sample - loss: 0.5624 - accuracy: 0.7511\n",
            "Epoch 61/300\n",
            "237/237 [==============================] - 0s 216us/sample - loss: 0.5336 - accuracy: 0.7511\n",
            "Epoch 62/300\n",
            "237/237 [==============================] - 0s 222us/sample - loss: 0.5124 - accuracy: 0.7595\n",
            "Epoch 63/300\n",
            "237/237 [==============================] - 0s 214us/sample - loss: 0.5093 - accuracy: 0.7764\n",
            "Epoch 64/300\n",
            "237/237 [==============================] - 0s 227us/sample - loss: 0.5231 - accuracy: 0.7764\n",
            "Epoch 65/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.5111 - accuracy: 0.7806\n",
            "Epoch 66/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.5878 - accuracy: 0.7890\n",
            "Epoch 67/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.5098 - accuracy: 0.8017\n",
            "Epoch 68/300\n",
            "237/237 [==============================] - 0s 196us/sample - loss: 0.5073 - accuracy: 0.7764\n",
            "Epoch 69/300\n",
            "237/237 [==============================] - 0s 208us/sample - loss: 0.4899 - accuracy: 0.7848\n",
            "Epoch 70/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.4526 - accuracy: 0.8101\n",
            "Epoch 71/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.5051 - accuracy: 0.8143\n",
            "Epoch 72/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.5337 - accuracy: 0.7637\n",
            "Epoch 73/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.4964 - accuracy: 0.7975\n",
            "Epoch 74/300\n",
            "237/237 [==============================] - 0s 197us/sample - loss: 0.4888 - accuracy: 0.7932\n",
            "Epoch 75/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.5109 - accuracy: 0.7806\n",
            "Epoch 76/300\n",
            "237/237 [==============================] - 0s 220us/sample - loss: 0.4919 - accuracy: 0.8017\n",
            "Epoch 77/300\n",
            "237/237 [==============================] - 0s 233us/sample - loss: 0.4769 - accuracy: 0.7848\n",
            "Epoch 78/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.4585 - accuracy: 0.8143\n",
            "Epoch 79/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.5347 - accuracy: 0.7764\n",
            "Epoch 80/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.4406 - accuracy: 0.8439\n",
            "Epoch 81/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.4705 - accuracy: 0.8397\n",
            "Epoch 82/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.4736 - accuracy: 0.7890\n",
            "Epoch 83/300\n",
            "237/237 [==============================] - 0s 197us/sample - loss: 0.4885 - accuracy: 0.8059\n",
            "Epoch 84/300\n",
            "237/237 [==============================] - 0s 214us/sample - loss: 0.4437 - accuracy: 0.8354\n",
            "Epoch 85/300\n",
            "237/237 [==============================] - 0s 237us/sample - loss: 0.5038 - accuracy: 0.8270\n",
            "Epoch 86/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.5027 - accuracy: 0.7932\n",
            "Epoch 87/300\n",
            "237/237 [==============================] - 0s 200us/sample - loss: 0.4206 - accuracy: 0.8481\n",
            "Epoch 88/300\n",
            "237/237 [==============================] - 0s 231us/sample - loss: 0.4716 - accuracy: 0.7848\n",
            "Epoch 89/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.3948 - accuracy: 0.8312\n",
            "Epoch 90/300\n",
            "237/237 [==============================] - 0s 197us/sample - loss: 0.4517 - accuracy: 0.8186\n",
            "Epoch 91/300\n",
            "237/237 [==============================] - 0s 217us/sample - loss: 0.4324 - accuracy: 0.8270\n",
            "Epoch 92/300\n",
            "237/237 [==============================] - 0s 219us/sample - loss: 0.4299 - accuracy: 0.8270\n",
            "Epoch 93/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.4755 - accuracy: 0.8397\n",
            "Epoch 94/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.4472 - accuracy: 0.7932\n",
            "Epoch 95/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.4537 - accuracy: 0.8270\n",
            "Epoch 96/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.3870 - accuracy: 0.8523\n",
            "Epoch 97/300\n",
            "237/237 [==============================] - 0s 225us/sample - loss: 0.4832 - accuracy: 0.8186\n",
            "Epoch 98/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.4166 - accuracy: 0.8186\n",
            "Epoch 99/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 0.3702 - accuracy: 0.8481\n",
            "Epoch 100/300\n",
            "237/237 [==============================] - 0s 196us/sample - loss: 0.4385 - accuracy: 0.8397\n",
            "Epoch 101/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 0.4623 - accuracy: 0.8565\n",
            "Epoch 102/300\n",
            "237/237 [==============================] - 0s 251us/sample - loss: 0.4325 - accuracy: 0.8608\n",
            "Epoch 103/300\n",
            "237/237 [==============================] - 0s 196us/sample - loss: 0.4280 - accuracy: 0.8523\n",
            "Epoch 104/300\n",
            "237/237 [==============================] - 0s 200us/sample - loss: 0.4273 - accuracy: 0.8186\n",
            "Epoch 105/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.4409 - accuracy: 0.8439\n",
            "Epoch 106/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.4368 - accuracy: 0.8354\n",
            "Epoch 107/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.4154 - accuracy: 0.8186\n",
            "Epoch 108/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.4782 - accuracy: 0.8270\n",
            "Epoch 109/300\n",
            "237/237 [==============================] - 0s 206us/sample - loss: 0.3885 - accuracy: 0.8565\n",
            "Epoch 110/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.3902 - accuracy: 0.8523\n",
            "Epoch 111/300\n",
            "237/237 [==============================] - 0s 219us/sample - loss: 0.3968 - accuracy: 0.8523\n",
            "Epoch 112/300\n",
            "237/237 [==============================] - 0s 226us/sample - loss: 0.4035 - accuracy: 0.8523\n",
            "Epoch 113/300\n",
            "237/237 [==============================] - 0s 216us/sample - loss: 0.4109 - accuracy: 0.8059\n",
            "Epoch 114/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.3875 - accuracy: 0.8270\n",
            "Epoch 115/300\n",
            "237/237 [==============================] - 0s 227us/sample - loss: 0.4050 - accuracy: 0.8439\n",
            "Epoch 116/300\n",
            "237/237 [==============================] - 0s 197us/sample - loss: 0.4229 - accuracy: 0.8523\n",
            "Epoch 117/300\n",
            "237/237 [==============================] - 0s 197us/sample - loss: 0.4175 - accuracy: 0.8186\n",
            "Epoch 118/300\n",
            "237/237 [==============================] - 0s 206us/sample - loss: 0.3697 - accuracy: 0.8650\n",
            "Epoch 119/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.3806 - accuracy: 0.8439\n",
            "Epoch 120/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.3586 - accuracy: 0.8608\n",
            "Epoch 121/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.4160 - accuracy: 0.8397\n",
            "Epoch 122/300\n",
            "237/237 [==============================] - 0s 197us/sample - loss: 0.4214 - accuracy: 0.8397\n",
            "Epoch 123/300\n",
            "237/237 [==============================] - 0s 219us/sample - loss: 0.3527 - accuracy: 0.8734\n",
            "Epoch 124/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.3244 - accuracy: 0.8819\n",
            "Epoch 125/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.4262 - accuracy: 0.8397\n",
            "Epoch 126/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.3836 - accuracy: 0.8312\n",
            "Epoch 127/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.3953 - accuracy: 0.8565\n",
            "Epoch 128/300\n",
            "237/237 [==============================] - 0s 215us/sample - loss: 0.4006 - accuracy: 0.8439\n",
            "Epoch 129/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.3434 - accuracy: 0.8861\n",
            "Epoch 130/300\n",
            "237/237 [==============================] - 0s 200us/sample - loss: 0.3310 - accuracy: 0.8692\n",
            "Epoch 131/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.4126 - accuracy: 0.8312\n",
            "Epoch 132/300\n",
            "237/237 [==============================] - 0s 219us/sample - loss: 0.3524 - accuracy: 0.8734\n",
            "Epoch 133/300\n",
            "237/237 [==============================] - 0s 215us/sample - loss: 0.3645 - accuracy: 0.8523\n",
            "Epoch 134/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.3762 - accuracy: 0.8608\n",
            "Epoch 135/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.3526 - accuracy: 0.8819\n",
            "Epoch 136/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.3274 - accuracy: 0.8692\n",
            "Epoch 137/300\n",
            "237/237 [==============================] - 0s 214us/sample - loss: 0.4724 - accuracy: 0.8734\n",
            "Epoch 138/300\n",
            "237/237 [==============================] - 0s 217us/sample - loss: 0.3567 - accuracy: 0.8650\n",
            "Epoch 139/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.3902 - accuracy: 0.8354\n",
            "Epoch 140/300\n",
            "237/237 [==============================] - 0s 215us/sample - loss: 0.4264 - accuracy: 0.8101\n",
            "Epoch 141/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.3934 - accuracy: 0.8354\n",
            "Epoch 142/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.3406 - accuracy: 0.8734\n",
            "Epoch 143/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.3778 - accuracy: 0.8692\n",
            "Epoch 144/300\n",
            "237/237 [==============================] - 0s 221us/sample - loss: 0.4044 - accuracy: 0.8439\n",
            "Epoch 145/300\n",
            "237/237 [==============================] - 0s 221us/sample - loss: 0.3816 - accuracy: 0.8565\n",
            "Epoch 146/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.3652 - accuracy: 0.8523\n",
            "Epoch 147/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.3615 - accuracy: 0.8608\n",
            "Epoch 148/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.3415 - accuracy: 0.8692\n",
            "Epoch 149/300\n",
            "237/237 [==============================] - 0s 236us/sample - loss: 0.3568 - accuracy: 0.8734\n",
            "Epoch 150/300\n",
            "237/237 [==============================] - 0s 188us/sample - loss: 0.3725 - accuracy: 0.8776\n",
            "Epoch 151/300\n",
            "237/237 [==============================] - 0s 217us/sample - loss: 0.3579 - accuracy: 0.8734\n",
            "Epoch 152/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.3572 - accuracy: 0.8650\n",
            "Epoch 153/300\n",
            "237/237 [==============================] - 0s 203us/sample - loss: 0.3319 - accuracy: 0.9030\n",
            "Epoch 154/300\n",
            "237/237 [==============================] - 0s 208us/sample - loss: 0.3776 - accuracy: 0.8481\n",
            "Epoch 155/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.3493 - accuracy: 0.8734\n",
            "Epoch 156/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.4166 - accuracy: 0.8354\n",
            "Epoch 157/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.3073 - accuracy: 0.8692\n",
            "Epoch 158/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.3003 - accuracy: 0.8776\n",
            "Epoch 159/300\n",
            "237/237 [==============================] - 0s 216us/sample - loss: 0.2969 - accuracy: 0.8861\n",
            "Epoch 160/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.2840 - accuracy: 0.8861\n",
            "Epoch 161/300\n",
            "237/237 [==============================] - 0s 214us/sample - loss: 0.2943 - accuracy: 0.8861\n",
            "Epoch 162/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.2885 - accuracy: 0.8819\n",
            "Epoch 163/300\n",
            "237/237 [==============================] - 0s 227us/sample - loss: 0.3029 - accuracy: 0.8903\n",
            "Epoch 164/300\n",
            "237/237 [==============================] - 0s 206us/sample - loss: 0.3123 - accuracy: 0.8987\n",
            "Epoch 165/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.3457 - accuracy: 0.8734\n",
            "Epoch 166/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.3719 - accuracy: 0.8945\n",
            "Epoch 167/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.3144 - accuracy: 0.8692\n",
            "Epoch 168/300\n",
            "237/237 [==============================] - 0s 198us/sample - loss: 0.3244 - accuracy: 0.8819\n",
            "Epoch 169/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.3266 - accuracy: 0.8523\n",
            "Epoch 170/300\n",
            "237/237 [==============================] - 0s 198us/sample - loss: 0.4557 - accuracy: 0.8481\n",
            "Epoch 171/300\n",
            "237/237 [==============================] - 0s 293us/sample - loss: 0.3438 - accuracy: 0.8734\n",
            "Epoch 172/300\n",
            "237/237 [==============================] - 0s 230us/sample - loss: 0.3180 - accuracy: 0.8650\n",
            "Epoch 173/300\n",
            "237/237 [==============================] - 0s 214us/sample - loss: 0.2687 - accuracy: 0.9198\n",
            "Epoch 174/300\n",
            "237/237 [==============================] - 0s 216us/sample - loss: 0.3336 - accuracy: 0.8903\n",
            "Epoch 175/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.3049 - accuracy: 0.8987\n",
            "Epoch 176/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.2810 - accuracy: 0.8819\n",
            "Epoch 177/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.2913 - accuracy: 0.9030\n",
            "Epoch 178/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.2822 - accuracy: 0.8734\n",
            "Epoch 179/300\n",
            "237/237 [==============================] - 0s 206us/sample - loss: 0.3268 - accuracy: 0.8819\n",
            "Epoch 180/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.3025 - accuracy: 0.8819\n",
            "Epoch 181/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.2813 - accuracy: 0.8903\n",
            "Epoch 182/300\n",
            "237/237 [==============================] - 0s 241us/sample - loss: 0.2680 - accuracy: 0.9072\n",
            "Epoch 183/300\n",
            "237/237 [==============================] - 0s 217us/sample - loss: 0.2879 - accuracy: 0.8861\n",
            "Epoch 184/300\n",
            "237/237 [==============================] - 0s 219us/sample - loss: 0.3239 - accuracy: 0.8945\n",
            "Epoch 185/300\n",
            "237/237 [==============================] - 0s 215us/sample - loss: 0.3186 - accuracy: 0.8565\n",
            "Epoch 186/300\n",
            "237/237 [==============================] - 0s 220us/sample - loss: 0.2967 - accuracy: 0.8734\n",
            "Epoch 187/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.2981 - accuracy: 0.9030\n",
            "Epoch 188/300\n",
            "237/237 [==============================] - 0s 223us/sample - loss: 0.3198 - accuracy: 0.8692\n",
            "Epoch 189/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.2904 - accuracy: 0.8987\n",
            "Epoch 190/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.3537 - accuracy: 0.8987\n",
            "Epoch 191/300\n",
            "237/237 [==============================] - 0s 243us/sample - loss: 0.3261 - accuracy: 0.8776\n",
            "Epoch 192/300\n",
            "237/237 [==============================] - 0s 203us/sample - loss: 0.3183 - accuracy: 0.8650\n",
            "Epoch 193/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.2781 - accuracy: 0.9030\n",
            "Epoch 194/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.2885 - accuracy: 0.9114\n",
            "Epoch 195/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.3344 - accuracy: 0.8692\n",
            "Epoch 196/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.2765 - accuracy: 0.9114\n",
            "Epoch 197/300\n",
            "237/237 [==============================] - 0s 221us/sample - loss: 0.2960 - accuracy: 0.9114\n",
            "Epoch 198/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.2862 - accuracy: 0.8945\n",
            "Epoch 199/300\n",
            "237/237 [==============================] - 0s 233us/sample - loss: 0.3516 - accuracy: 0.8945\n",
            "Epoch 200/300\n",
            "237/237 [==============================] - 0s 208us/sample - loss: 0.3450 - accuracy: 0.8776\n",
            "Epoch 201/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.2814 - accuracy: 0.9072\n",
            "Epoch 202/300\n",
            "237/237 [==============================] - 0s 250us/sample - loss: 0.2622 - accuracy: 0.9030\n",
            "Epoch 203/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.2565 - accuracy: 0.8945\n",
            "Epoch 204/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.2647 - accuracy: 0.9030\n",
            "Epoch 205/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.3584 - accuracy: 0.8819\n",
            "Epoch 206/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.3161 - accuracy: 0.8903\n",
            "Epoch 207/300\n",
            "237/237 [==============================] - 0s 203us/sample - loss: 0.4186 - accuracy: 0.8650\n",
            "Epoch 208/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.3656 - accuracy: 0.8650\n",
            "Epoch 209/300\n",
            "237/237 [==============================] - 0s 217us/sample - loss: 0.2991 - accuracy: 0.8861\n",
            "Epoch 210/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.3025 - accuracy: 0.9030\n",
            "Epoch 211/300\n",
            "237/237 [==============================] - 0s 208us/sample - loss: 0.3167 - accuracy: 0.8903\n",
            "Epoch 212/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.3209 - accuracy: 0.8945\n",
            "Epoch 213/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.2727 - accuracy: 0.8945\n",
            "Epoch 214/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.2710 - accuracy: 0.8945\n",
            "Epoch 215/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.2461 - accuracy: 0.9198\n",
            "Epoch 216/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.2398 - accuracy: 0.9114\n",
            "Epoch 217/300\n",
            "237/237 [==============================] - 0s 233us/sample - loss: 0.2423 - accuracy: 0.9198\n",
            "Epoch 218/300\n",
            "237/237 [==============================] - 0s 192us/sample - loss: 0.2645 - accuracy: 0.9114\n",
            "Epoch 219/300\n",
            "237/237 [==============================] - 0s 217us/sample - loss: 0.2689 - accuracy: 0.9030\n",
            "Epoch 220/300\n",
            "237/237 [==============================] - 0s 198us/sample - loss: 0.3373 - accuracy: 0.8650\n",
            "Epoch 221/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.2985 - accuracy: 0.8903\n",
            "Epoch 222/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.2649 - accuracy: 0.9241\n",
            "Epoch 223/300\n",
            "237/237 [==============================] - 0s 243us/sample - loss: 0.3094 - accuracy: 0.8734\n",
            "Epoch 224/300\n",
            "237/237 [==============================] - 0s 243us/sample - loss: 0.3418 - accuracy: 0.8987\n",
            "Epoch 225/300\n",
            "237/237 [==============================] - 0s 191us/sample - loss: 0.2808 - accuracy: 0.8903\n",
            "Epoch 226/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.2894 - accuracy: 0.8861\n",
            "Epoch 227/300\n",
            "237/237 [==============================] - 0s 195us/sample - loss: 0.2768 - accuracy: 0.8987\n",
            "Epoch 228/300\n",
            "237/237 [==============================] - 0s 214us/sample - loss: 0.2406 - accuracy: 0.9114\n",
            "Epoch 229/300\n",
            "237/237 [==============================] - 0s 190us/sample - loss: 0.3199 - accuracy: 0.9156\n",
            "Epoch 230/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.3011 - accuracy: 0.8861\n",
            "Epoch 231/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.2716 - accuracy: 0.9030\n",
            "Epoch 232/300\n",
            "237/237 [==============================] - 0s 202us/sample - loss: 0.2629 - accuracy: 0.8987\n",
            "Epoch 233/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.2247 - accuracy: 0.9156\n",
            "Epoch 234/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.3079 - accuracy: 0.8776\n",
            "Epoch 235/300\n",
            "237/237 [==============================] - 0s 194us/sample - loss: 0.2484 - accuracy: 0.9030\n",
            "Epoch 236/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.2583 - accuracy: 0.8945\n",
            "Epoch 237/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.2815 - accuracy: 0.8819\n",
            "Epoch 238/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.2589 - accuracy: 0.9198\n",
            "Epoch 239/300\n",
            "237/237 [==============================] - 0s 206us/sample - loss: 0.2697 - accuracy: 0.8903\n",
            "Epoch 240/300\n",
            "237/237 [==============================] - 0s 216us/sample - loss: 0.3172 - accuracy: 0.8987\n",
            "Epoch 241/300\n",
            "237/237 [==============================] - 0s 204us/sample - loss: 0.2548 - accuracy: 0.9241\n",
            "Epoch 242/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.2693 - accuracy: 0.9114\n",
            "Epoch 243/300\n",
            "237/237 [==============================] - 0s 199us/sample - loss: 0.2520 - accuracy: 0.8903\n",
            "Epoch 244/300\n",
            "237/237 [==============================] - 0s 221us/sample - loss: 0.2458 - accuracy: 0.9114\n",
            "Epoch 245/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.2292 - accuracy: 0.9198\n",
            "Epoch 246/300\n",
            "237/237 [==============================] - 0s 222us/sample - loss: 0.2622 - accuracy: 0.9156\n",
            "Epoch 247/300\n",
            "237/237 [==============================] - 0s 238us/sample - loss: 0.2475 - accuracy: 0.8987\n",
            "Epoch 248/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.2938 - accuracy: 0.8987\n",
            "Epoch 249/300\n",
            "237/237 [==============================] - 0s 222us/sample - loss: 0.2613 - accuracy: 0.8987\n",
            "Epoch 250/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.2429 - accuracy: 0.9072\n",
            "Epoch 251/300\n",
            "237/237 [==============================] - 0s 215us/sample - loss: 0.2591 - accuracy: 0.9030\n",
            "Epoch 252/300\n",
            "237/237 [==============================] - 0s 218us/sample - loss: 0.3197 - accuracy: 0.8776\n",
            "Epoch 253/300\n",
            "237/237 [==============================] - 0s 219us/sample - loss: 0.2546 - accuracy: 0.8903\n",
            "Epoch 254/300\n",
            "237/237 [==============================] - 0s 223us/sample - loss: 0.2554 - accuracy: 0.8903\n",
            "Epoch 255/300\n",
            "237/237 [==============================] - 0s 233us/sample - loss: 0.2107 - accuracy: 0.9283\n",
            "Epoch 256/300\n",
            "237/237 [==============================] - 0s 232us/sample - loss: 0.1982 - accuracy: 0.9283\n",
            "Epoch 257/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.3060 - accuracy: 0.8861\n",
            "Epoch 258/300\n",
            "237/237 [==============================] - 0s 209us/sample - loss: 0.2911 - accuracy: 0.8945\n",
            "Epoch 259/300\n",
            "237/237 [==============================] - 0s 208us/sample - loss: 0.2489 - accuracy: 0.9156\n",
            "Epoch 260/300\n",
            "237/237 [==============================] - 0s 234us/sample - loss: 0.2315 - accuracy: 0.9030\n",
            "Epoch 261/300\n",
            "237/237 [==============================] - 0s 216us/sample - loss: 0.2676 - accuracy: 0.9114\n",
            "Epoch 262/300\n",
            "237/237 [==============================] - 0s 232us/sample - loss: 0.2431 - accuracy: 0.9072\n",
            "Epoch 263/300\n",
            "237/237 [==============================] - 0s 220us/sample - loss: 0.2649 - accuracy: 0.9198\n",
            "Epoch 264/300\n",
            "237/237 [==============================] - 0s 242us/sample - loss: 0.2766 - accuracy: 0.9156\n",
            "Epoch 265/300\n",
            "237/237 [==============================] - 0s 271us/sample - loss: 0.2462 - accuracy: 0.8987\n",
            "Epoch 266/300\n",
            "237/237 [==============================] - 0s 218us/sample - loss: 0.2285 - accuracy: 0.9114\n",
            "Epoch 267/300\n",
            "237/237 [==============================] - 0s 231us/sample - loss: 0.2330 - accuracy: 0.9030\n",
            "Epoch 268/300\n",
            "237/237 [==============================] - 0s 205us/sample - loss: 0.2262 - accuracy: 0.9072\n",
            "Epoch 269/300\n",
            "237/237 [==============================] - 0s 229us/sample - loss: 0.2006 - accuracy: 0.9325\n",
            "Epoch 270/300\n",
            "237/237 [==============================] - 0s 218us/sample - loss: 0.3395 - accuracy: 0.8776\n",
            "Epoch 271/300\n",
            "237/237 [==============================] - 0s 220us/sample - loss: 0.2516 - accuracy: 0.8987\n",
            "Epoch 272/300\n",
            "237/237 [==============================] - 0s 217us/sample - loss: 0.2851 - accuracy: 0.8987\n",
            "Epoch 273/300\n",
            "237/237 [==============================] - 0s 223us/sample - loss: 0.2754 - accuracy: 0.8945\n",
            "Epoch 274/300\n",
            "237/237 [==============================] - 0s 210us/sample - loss: 0.2582 - accuracy: 0.9114\n",
            "Epoch 275/300\n",
            "237/237 [==============================] - 0s 201us/sample - loss: 0.2445 - accuracy: 0.9114\n",
            "Epoch 276/300\n",
            "237/237 [==============================] - 0s 227us/sample - loss: 0.2330 - accuracy: 0.8903\n",
            "Epoch 277/300\n",
            "237/237 [==============================] - 0s 217us/sample - loss: 0.2643 - accuracy: 0.9156\n",
            "Epoch 278/300\n",
            "237/237 [==============================] - 0s 224us/sample - loss: 0.2237 - accuracy: 0.9114\n",
            "Epoch 279/300\n",
            "237/237 [==============================] - 0s 214us/sample - loss: 0.2180 - accuracy: 0.9241\n",
            "Epoch 280/300\n",
            "237/237 [==============================] - 0s 212us/sample - loss: 0.2721 - accuracy: 0.9072\n",
            "Epoch 281/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.2848 - accuracy: 0.8692\n",
            "Epoch 282/300\n",
            "237/237 [==============================] - 0s 219us/sample - loss: 0.2689 - accuracy: 0.8987\n",
            "Epoch 283/300\n",
            "237/237 [==============================] - 0s 207us/sample - loss: 0.2337 - accuracy: 0.9156\n",
            "Epoch 284/300\n",
            "237/237 [==============================] - 0s 241us/sample - loss: 0.2449 - accuracy: 0.9156\n",
            "Epoch 285/300\n",
            "237/237 [==============================] - 0s 225us/sample - loss: 0.3266 - accuracy: 0.9072\n",
            "Epoch 286/300\n",
            "237/237 [==============================] - 0s 220us/sample - loss: 0.2570 - accuracy: 0.8945\n",
            "Epoch 287/300\n",
            "237/237 [==============================] - 0s 222us/sample - loss: 0.2377 - accuracy: 0.9198\n",
            "Epoch 288/300\n",
            "237/237 [==============================] - 0s 206us/sample - loss: 0.2444 - accuracy: 0.9030\n",
            "Epoch 289/300\n",
            "237/237 [==============================] - 0s 213us/sample - loss: 0.2379 - accuracy: 0.9198\n",
            "Epoch 290/300\n",
            "237/237 [==============================] - 0s 216us/sample - loss: 0.2285 - accuracy: 0.8987\n",
            "Epoch 291/300\n",
            "237/237 [==============================] - 0s 230us/sample - loss: 0.2084 - accuracy: 0.9283\n",
            "Epoch 292/300\n",
            "237/237 [==============================] - 0s 216us/sample - loss: 0.1795 - accuracy: 0.9367\n",
            "Epoch 293/300\n",
            "237/237 [==============================] - 0s 215us/sample - loss: 0.2200 - accuracy: 0.9072\n",
            "Epoch 294/300\n",
            "237/237 [==============================] - 0s 252us/sample - loss: 0.2582 - accuracy: 0.9156\n",
            "Epoch 295/300\n",
            "237/237 [==============================] - 0s 211us/sample - loss: 0.2739 - accuracy: 0.8861\n",
            "Epoch 296/300\n",
            "237/237 [==============================] - 0s 228us/sample - loss: 0.2436 - accuracy: 0.9156\n",
            "Epoch 297/300\n",
            "237/237 [==============================] - 0s 220us/sample - loss: 0.2561 - accuracy: 0.8987\n",
            "Epoch 298/300\n",
            "237/237 [==============================] - 0s 224us/sample - loss: 0.2275 - accuracy: 0.9072\n",
            "Epoch 299/300\n",
            "237/237 [==============================] - 0s 220us/sample - loss: 0.2843 - accuracy: 0.8987\n",
            "Epoch 300/300\n",
            "237/237 [==============================] - 0s 238us/sample - loss: 0.2441 - accuracy: 0.9114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7331c75e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4FmTgglHiIR",
        "colab_type": "text"
      },
      "source": [
        "#### Training accuracy approaches 100%, however when we run the model on our test set, we only get 77% accuracy, which suggest we have some overfitting to our training set, most likely because our patient dataset is very small as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4G37KSAyilA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1b116c5c-39ef-4a2b-c594-01eb1d800213"
      },
      "source": [
        "test_loss, test_accuracy = model_binary.evaluate(X_test, Y_test_binary)\n",
        "print(\"Test Patient Accuracy: {}\".format(test_accuracy))\n"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 0s 2ms/sample - loss: 1.4566 - accuracy: 0.7667\n",
            "Test Patient Accuracy: 0.7666666507720947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGTnm0zNylII",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing Results: Precision, Recall, and F1 Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3BqAmVHIx39",
        "colab_type": "text"
      },
      "source": [
        "### State of the art machine learning models typically have F1 scores in the greater than 90% range. F1 Score is in the weighted average between our precision (measure of number of false postives) and Recall (measure of the number of false negatives, which for heart disease is more dangerous). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDZMUkd8t6FG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "12f4ff5c-e498-4c57-829f-c53576297fe2"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "categorical_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "print(\"Result for Categorical Model: \")\n",
        "print(accuracy_score(y_test, categorical_pred))\n",
        "print(classification_report(y_test, categorical_pred))"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result for Categorical Model: \n",
            "0.6333333333333333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.88        38\n",
            "           1       0.17      0.12      0.14         8\n",
            "           2       0.25      0.33      0.29         6\n",
            "           3       0.00      0.00      0.00         6\n",
            "           4       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.63        60\n",
            "   macro avg       0.25      0.28      0.26        60\n",
            "weighted avg       0.57      0.63      0.60        60\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIlXIt-zI1Bq",
        "colab_type": "text"
      },
      "source": [
        "### 60% on an F1-Score, not very good for predicting what type of heart disease a patient has."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is9GAgG5zQNJ",
        "colab_type": "text"
      },
      "source": [
        "### Our F1 Score however is much better when converted to a binary classification problem, 76%, (Does the patient have heart disease?), however its still far from being a completely reliable diagnosing tool given our dataset. We'd need to do some more feature engineering and data collection to train this model better. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiEQpFBVudke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3d6ece58-0131-4494-d902-797ddb56f29f"
      },
      "source": [
        "# Preccsion tracks false positives, Recall tracks false negatives\n",
        "\n",
        "binary_pred = np.round(model_binary.predict(X_test)).astype(int)\n",
        "\n",
        "print(\"Result for Binary Model: \")\n",
        "print(accuracy_score(Y_test_binary, binary_pred))\n",
        "print(classification_report(Y_test_binary, binary_pred))"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result for Binary Model: \n",
            "0.7666666666666667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82        38\n",
            "           1       0.70      0.64      0.67        22\n",
            "\n",
            "    accuracy                           0.77        60\n",
            "   macro avg       0.75      0.74      0.74        60\n",
            "weighted avg       0.76      0.77      0.76        60\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Ffe8aJyra_",
        "colab_type": "text"
      },
      "source": [
        "### A cardiologist could use this model right now as a pointer towards a probability of a patient having heart disease, however it shouldn't be used as the end-all-be-all of a diagnosis. We'd need more patient data and more features to make it more accurate."
      ]
    }
  ]
}